{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import matplotlib # Solve \"RuntimeError\" on mac OSX\n",
    "# matplotlib.use('TkAgg') # Solve \"RuntimeError\" on mac OSX\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/iconography'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d01081393679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shuffle data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-d01081393679>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mclass_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCATEGORIES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get the classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# iterate over each image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 img_array = cv2.imread(os.path.join(path,img),\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/iconography'"
     ]
    }
   ],
   "source": [
    "DATADIR = 'dataset/'\n",
    "CATEGORIES = ['iconography', 'painting', 'sculpture']\n",
    "IMG_SIZE = 50\n",
    "\n",
    "dataset = []\n",
    "\n",
    "def create_dataset():\n",
    "    for category in CATEGORIES:  \n",
    "        \n",
    "        path = os.path.join(DATADIR,category)  # create path \n",
    "        class_num = CATEGORIES.index(category)  # get the classification \n",
    "\n",
    "        for img in tqdm(os.listdir(path)):  # iterate over each image \n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img),\n",
    "                                       cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "                new_array = cv2.resize(img_array, \n",
    "                                        (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
    "                dataset.append([new_array, \n",
    "                                class_num])  # add this to our training_data\n",
    "            except Exception as e:  \n",
    "                pass\n",
    "\n",
    "create_dataset()        \n",
    "\n",
    "random.shuffle(dataset) # shuffle data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for features,label in dataset:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X).reshape(-1, \n",
    "                        IMG_SIZE, IMG_SIZE, \n",
    "                        1)\n",
    "\n",
    "# Split into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) \n",
    "\n",
    "# Normalization\n",
    "X_train = X_train/255.0 \n",
    "X_test = X_test/255.0\n",
    "\n",
    "# Convert class vector to binary class matrix\n",
    "y_train = to_categorical(y_train, num_classes=3)\n",
    "y_test = to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "num_classes = 3\n",
    "dense_layer = 1\n",
    "layer_size = 256\n",
    "conv_layer = 2\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(layer_size, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(layer_size, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(layer_size))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "4774/4774 [==============================] - 96s 20ms/step - loss: 0.6150 - acc: 0.7421\n",
      "Epoch 2/4\n",
      "4774/4774 [==============================] - 99s 21ms/step - loss: 0.3643 - acc: 0.8641\n",
      "Epoch 3/4\n",
      "4774/4774 [==============================] - 102s 21ms/step - loss: 0.3041 - acc: 0.8877\n",
      "Epoch 4/4\n",
      "4774/4774 [==============================] - 98s 20ms/step - loss: 0.2518 - acc: 0.9064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3902bc18>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_EPOCH = 4\n",
    "BATCH_SIZE = 32\n",
    "VERBOSE = 1\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          epochs=NB_EPOCH, \n",
    "          batch_size=BATCH_SIZE,          \n",
    "          verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194/1194 [==============================] - 6s 5ms/step\n",
      "loss: 0.3452226603949689\n",
      "accuracy: 0.8735343382586187\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('loss:', test_loss) \n",
    "print('accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
